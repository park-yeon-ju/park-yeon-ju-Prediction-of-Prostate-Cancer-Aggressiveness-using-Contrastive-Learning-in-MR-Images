{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch\n",
        "!pip3 install torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lycFQDXsEdxp",
        "outputId": "f347600f-95a2-4fbd-b9d6-8387a79f7f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.1+cu113)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn3LBM38EY6t",
        "outputId": "7c0bb31c-81db-4e84-ab4a-36390c5d92bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import sys\n",
        "import yaml\n",
        "from torchvision import transforms, datasets\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import tensorflow as tf\n",
        "\n",
        "#구글 드라이브에 있는 폴더 import 할 수 있도록 권한 부여\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-Ljd39AE7-w",
        "outputId": "45304b40-7d58-4f97-8c42-11a0309b91c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82GhROmmEY6w"
      },
      "outputs": [],
      "source": [
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/yeonju_byol/PyTorch-BYOL')\n",
        "from models.resnet_base_network import ResNet18\n",
        "# ResNet18 네트워크 import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KqkCVriEY6x"
      },
      "outputs": [],
      "source": [
        "batch_size = 512 \n",
        "# dataloader 코드에서 사용됨\n",
        "\n",
        "data_transforms = torchvision.transforms.Compose([transforms.ToTensor()]) \n",
        "# 데이터를 tensor로 바꾸는 과정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC9BSmWBEY6x",
        "outputId": "9e13f8b2-8ce1-4f02-8c97-d313fb67a433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'network': {'name': 'resnet18', 'fine_tune_from': 'resnet-18_40-epochs', 'projection_head': {'mlp_hidden_size': 512, 'projection_size': 128}}, 'data_transforms': {'s': 1, 'input_shape': '(96,96,3)'}, 'trainer': {'batch_size': 256, 'm': 0.996, 'checkpoint_interval': 5000, 'max_epochs': 400, 'num_workers': 4}, 'optimizer': {'params': {'lr': 0.03, 'momentum': 0.9, 'weight_decay': 0.0004}}}\n"
          ]
        }
      ],
      "source": [
        "config = yaml.load(open(\"/content/drive/MyDrive/Colab Notebooks/yeonju_byol/PyTorch-BYOL/config/config.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
        "# config파일 로드\n",
        "# yaml.load() 함수로 YAML 파일을 파싱하여 config 라는 이름의 Python 객체로 저장\n",
        "\n",
        "print(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKwIQ0ymEY6y",
        "outputId": "f38b1d6a-9bf2-4bba-c2a2-ed2cbcc30b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "train_dataset = datasets.STL10('/content/drive/MyDrive/Colab Notebooks/yeonju_byol/Downloads/', split='train', download=True,\n",
        "                               transform=data_transforms)\n",
        "\n",
        "test_dataset = datasets.STL10('/content/drive/MyDrive/Colab Notebooks/yeonju_byol/Downloads/', split='test', download=True,\n",
        "                               transform=data_transforms) \n",
        "\n",
        "# 최초 다운로드 시에만 downlad=True로 변경\n",
        "# datasets 정의\n",
        "# split에 쓴 것에 따라 데이터 세트가 결정됨\n",
        "# transform은 PIL 이미지를 가져와 변환된 버전을 반환해줌"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJ0pFOnWEY6y",
        "outputId": "6edce8d9-99dc-4858-d18c-cb73f3aaab9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([3, 96, 96])\n"
          ]
        }
      ],
      "source": [
        "print(\"Input shape:\", train_dataset[0][0].shape)\n",
        "# train_dataset의 shape은 크게 3개의 배열로 묶여있고 그 안은 96*96 배열의 모습을 취함.\n",
        "# 즉, 96*96 3차원 이미지 파일 -> color니까"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufhdsJkTEY6z"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                          num_workers=0, drop_last=False, shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                          num_workers=0, drop_last=False, shuffle=True)\n",
        "\n",
        "# loader에 데이터셋을 불러오는 과정\n",
        "# train_dataset/test_dataset : dataset은 각 train과 test dataset\n",
        "# batch_size: 1회 당 batch_size개의 데이터씩 분할\n",
        "# num_workers : default 값은 0이고, data 로딩을 위해 몇 개의 서브 프로세스를 사용할 것인지를 결정하는 파라미터\n",
        "# drop_last : batch의 길이가 다른 경우에 따라 loss를 구하기 귀찮은 경우가 생기고, batch의 크기에 따른 의존도 높은 함수를 사용할 때 걱정이 되는 경우 마지막 batch를 사용하지 않을 수 있게 하는 파라미터\n",
        "# shuffle : 데이터들의 순서는 섞어서 분할"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsW0HbHUEY60",
        "outputId": "24073166-1a54-4520-d335-94011e872078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "device = 'cpu' #'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "encoder = ResNet18(**config['network'])\n",
        "# byol 인코더 네트워크 설정\n",
        "# 불러올 모델 설정\n",
        "\n",
        "output_feature_dim = encoder.projetion.net[0].in_features\n",
        "# =512\n",
        "# resnet18의 마지막 노드 개수? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFtdDbEGEY60",
        "outputId": "125f673b-e0d9-4552-f889-c0e480af9130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters successfully loaded.\n"
          ]
        }
      ],
      "source": [
        "#load pre-trained parameters\n",
        "load_params = torch.load(os.path.join('/content/drive/MyDrive/Colab Notebooks/yeonju_byol/PyTorch-BYOL/runs/b 256, e 400 _ 2/checkpoints/model.pth'),\n",
        "                         map_location=torch.device(torch.device(device)))\n",
        "# runs폴더에서 불러올 Training된 model설정\n",
        "# pickle을 사용하여 저장된 객체 파일들을 역직렬화하여 메모리에 올리는 과정.\n",
        "\n",
        "# state_dict 는 간단히 말해 각 계층을 매개변수 텐서로 매핑되는 Python 사전(dict) 객체입니다.\n",
        "if 'online_network_state_dict' in load_params:\n",
        "    encoder.load_state_dict(load_params['online_network_state_dict'])\n",
        "    print(\"Parameters successfully loaded.\")\n",
        "    # 역직렬화된 state_dict 를 사용하여 모델의 매개변수(ex.가중치와 편향)들을 불러오는 과정\n",
        "    # path 파일에 있는 online_network 사전객체 매개변수들을 load.state_dict로 불러오는 것\n",
        "\n",
        "# remove the projection head\n",
        "encoder = torch.nn.Sequential(*list(encoder.children())[:-1])    \n",
        "# Sequential 객체는 그 안에 포함된 각 모듈을 순차적으로 실행함.   \n",
        "# [:-1] # 맨 마지막 값을 제외하고 모두\n",
        "# encoder에서 MLPHead 제거\n",
        "\n",
        "encoder = encoder.to(device)\n",
        "# 모델에서 사용하는 input Tensor들은 input = input.to(device) 을 호출해야 합니다.\n",
        "# GPU에서 학습한 모델을 GPU에서 불러올 때에는, 초기화된 model 에 model.to(torch.device('cuda')) 을 호출하여 CUDA 최적화된 모델로 변환해야함\n",
        "# 또한 모델에 데이터를 제공하는 모든 입력에 .to(torch.device('cuda')) 함수를 호출해야 함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWX73O_DEY61",
        "outputId": "3426cbac-b645-4a9b-8494-473f9dd844eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(encoder): Sequential(\n",
        "\n",
        "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "\t# 입력 이미지의 채널 수 : 3\n",
        "\t# 컨볼루션에 의해 생성된 채널 수 : 64\n",
        "\t# 컨볼루션 커널의 크기 : (7 , 7)\n",
        "\t# 컨볼루션의 스트라이드 : (2 , 2)\n",
        "\t# input의 모든 4면에 추가된 패딩 : (3 , 3)\n",
        "\t# output에 학습 가능한 편향을 추가하지 않음\n",
        "\n",
        "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "\t# (N, C, H, W) 크기의 기대 input의 채널의 수 : 64\n",
        "\t# 수치적 안정성을 위해 분모에 추가된 값 : 기본 값\n",
        "\t# running_mean 및 running_var 계산에 사용되는 값 : 기본 값\n",
        "\t# 이 모듈은 학습 가능한 affine 매개변수를 갖음\n",
        "\t# 이 모듈이 실행 평균과 분산을 추적함\n",
        "\n",
        "    (2): ReLU(inplace=True)\n",
        "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
        "\t# 최대를 넘길 수 있는 창의 크기 : 3\n",
        "\t# 보폭 of the window : 2\n",
        "\t# 양쪽에 추가할 암시적 제로 패딩 : 1\n",
        "\t# 창에서 요소의 보폭을 제어하는 매개변수 : 1\n",
        "\t# \n",
        "    (4): Sequential(\n",
        "      (0): BasicBlock(\n",
        "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        (relu): ReLU(inplace=True)\n",
        "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      )\n",
        "\n",
        "      .\n",
        "      .\n",
        "      .\n",
        "    (8): AdaptiveAvgPool2d(output_size=(1, 1))"
      ],
      "metadata": {
        "id": "O-V3GEr4Ugf-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iui1VPfMEY62"
      },
      "outputs": [],
      "source": [
        "# 로지스틱 회귀 클래스\n",
        "class LogisticRegression(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "        # nn.Linear 를 선형 레이어로 사용\n",
        "        # 들어오는 데이터에 대해 선형 변환을 적용: y = xA^T + b\n",
        "        # torch.nn.Linear(각 입력 샘플의 크기, 각 출력 샘플의 크기)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyW8jjCYEY62"
      },
      "outputs": [],
      "source": [
        "logreg = LogisticRegression(output_feature_dim, 10) # (512,10)\n",
        "logreg = logreg.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKC3Pou3EY62"
      },
      "outputs": [],
      "source": [
        "def get_features_from_encoder(encoder, loader):\n",
        "    \n",
        "    x_train = []\n",
        "    y_train = []\n",
        "\n",
        "    # get the features from the pre-trained model\n",
        "    # i는 인덱스, (x,y)는 원소\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        with torch.no_grad():\n",
        "            feature_vector = encoder(x)\n",
        "            x_train.extend(feature_vector) # feature_vector의 각 항목들을 x_train에 각각의 원소로 넣음.\n",
        "            y_train.extend(y.numpy()) # y를 넘파이 배열로 변환하여 y_train에 각각의 원소로 넣음.\n",
        "\n",
        "        # torch.no_grad : 그래디언트 계산을 비활성화하는 컨텍스트 관리자.\n",
        "\n",
        "            \n",
        "    x_train = torch.stack(x_train) # 새 차원을 따라 텐서 시퀀스를 연결. 연결할 때 마다 차원 추가됨.\n",
        "    y_train = torch.tensor(y_train) # y_train 데이터를 복사해 autograd 히스토리가 없는 텐서를 구성.\n",
        "    return x_train, y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89pVvKmTEY63",
        "outputId": "18c3022a-062e-4c2d-df04-8b06bd1bcf5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: torch.Size([5000, 512]) torch.Size([5000])\n",
            "Testing data shape: torch.Size([8000, 512]) torch.Size([8000])\n"
          ]
        }
      ],
      "source": [
        "encoder.eval() # evaluation 과정에서 사용하지 않아야 하는 layer들을 알아서 off 시키도록 하는 함수\n",
        "x_train, y_train = get_features_from_encoder(encoder, train_loader)\n",
        "x_test, y_test = get_features_from_encoder(encoder, test_loader)\n",
        "\n",
        "if len(x_train.shape) > 2:\n",
        "    x_train = torch.mean(x_train, dim=[2, 3]) # 차원 축소\n",
        "    x_test = torch.mean(x_test, dim=[2, 3]) # 차원 축소\n",
        "    \n",
        "print(\"Training data shape:\", x_train.shape, y_train.shape)\n",
        "print(\"Testing data shape:\", x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJku8SRqEY63"
      },
      "outputs": [],
      "source": [
        "def create_data_loaders_from_arrays(X_train, y_train, X_test, y_test):\n",
        "\n",
        "    train = torch.utils.data.TensorDataset(X_train, y_train) # Dataset wrapping tensors\n",
        "    train_loader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)\n",
        "\n",
        "    test = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "    test_loader = torch.utils.data.DataLoader(test, batch_size=512, shuffle=False)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML8lGODMEY63"
      },
      "outputs": [],
      "source": [
        "# 데이터 전처리\n",
        "scaler = preprocessing.StandardScaler() # 평균을 제거하고 단위 분산에 맞게 조정하여 features를 표준화\n",
        "scaler.fit(x_train) # x_train 데이터를 학습시키는 메서드\n",
        "x_train = scaler.transform(x_train).astype(np.float32) # 학습시킨 것을 x_train에 적용하는 메서드\n",
        "x_test = scaler.transform(x_test).astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-qP9G9CEY64"
      },
      "outputs": [],
      "source": [
        "train_loader, test_loader = create_data_loaders_from_arrays(torch.from_numpy(x_train), y_train, torch.from_numpy(x_test), y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GUWZbuVEY64",
        "outputId": "ed06fe06-2905-4f9d-c9c3-1a6defc87c7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing accuracy: 74.175\n",
            "Testing accuracy: 82.5\n",
            "Testing accuracy: 83.3875\n",
            "Testing accuracy: 83.625\n",
            "Testing accuracy: 83.575\n",
            "Testing accuracy: 83.3\n",
            "Testing accuracy: 83.175\n",
            "Testing accuracy: 82.9875\n",
            "Testing accuracy: 82.7625\n",
            "Testing accuracy: 82.6375\n",
            "Testing accuracy: 82.55\n",
            "Testing accuracy: 82.425\n",
            "Testing accuracy: 82.35\n",
            "Testing accuracy: 82.25\n",
            "Testing accuracy: 82.2125\n",
            "Testing accuracy: 82.2\n",
            "Testing accuracy: 82.0875\n",
            "Testing accuracy: 81.9375\n",
            "Testing accuracy: 81.9\n",
            "Testing accuracy: 81.625\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(logreg.parameters(), lr=3e-4) # adamoptimizer 최적화\n",
        "criterion = torch.nn.CrossEntropyLoss() # 기준 : CrossEntropyLoss (= 입력과 목표 사이의 교차 엔트로피 손실을 계산)\n",
        "eval_every_n_epochs = 10\n",
        "\n",
        "for epoch in range(200):\n",
        "#     train_acc = []\n",
        "    for x, y in train_loader:\n",
        "\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # 최적화 과정 (~ optimizer.step())\n",
        "        # zero the parameter gradients\n",
        "        # 모델 매개변수의 변화도를 재설정. \n",
        "        # 기본적으로 변화도는 더해지기(add up) 때문에 중복 계산을 막기 위해 반복할 때마다 명시적으로 0으로 설정.\n",
        "        optimizer.zero_grad()        \n",
        "        \n",
        "        logits = logreg(x)\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        # dimension (열)을 기준으로 input tensor 안에 있는 최대값의 위치를 각각 반환\n",
        "\n",
        "        # 손실(loss) 계산\n",
        "        loss = criterion(logits, y)\n",
        "        \n",
        "        loss.backward() # 예측 손실(prediction loss)을 역전파\n",
        "        optimizer.step() # 역전파 단계에서 수집된 변화도로 매개변수를 조정\n",
        "    \n",
        "    total = 0\n",
        "    if epoch % eval_every_n_epochs == 0:\n",
        "        correct = 0\n",
        "        for x, y in test_loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            logits = logreg(x)\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            \n",
        "            total += y.size(0)\n",
        "            correct += (predictions == y).sum().item()\n",
        "            \n",
        "        acc = 100 * correct / total\n",
        "        print(f\"Testing accuracy: {np.mean(acc)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "dzM6vHRsEY64",
        "outputId": "67e0aa45-77b5-4850-c9f1-1926be4a5f1e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#그래프를 그리기 위한 설정\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (8,8) #캔버스 가로 세로 길이 설정\n",
        "plt.title('Testing accuracy per 10 epochs')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('testing accuracy')\n",
        "#plt.legend(['test Accuracy',], loc='upper right')\n",
        "plt.grid()\n",
        "\n",
        "plt.plot(np.mean(acc))\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}